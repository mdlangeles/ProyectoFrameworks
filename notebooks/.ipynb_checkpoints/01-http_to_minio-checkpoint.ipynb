{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a86fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.destinations.filesystem import filesystem\n",
    "import pyarrow.parquet as pq\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import tempfile\n",
    "import pyarrow.parquet as pq\n",
    "from minio import Minio\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16a4d59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigFieldMissingException",
     "evalue": "Following fields are missing: ['endpoint_url'] in configuration with spec Any\n\tfor field \"endpoint_url\" config providers and keys were tried in following order:\n\t\tIn Environment Variables key ENDPOINT_URL was not found.\nPlease refer to https://dlthub.com/docs/general-usage/credentials for more information\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigFieldMissingException\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdlt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecrets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/configuration/accessors.py:23\u001b[0m, in \u001b[0;36m_Accessor.__getitem__\u001b[0;34m(self, field)\u001b[0m\n\u001b[1;32m     21\u001b[0m value, traces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(field)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigFieldMissingException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAny\u001b[39m\u001b[38;5;124m\"\u001b[39m, {field: traces})\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auto_cast(value)\n",
      "\u001b[0;31mConfigFieldMissingException\u001b[0m: Following fields are missing: ['endpoint_url'] in configuration with spec Any\n\tfor field \"endpoint_url\" config providers and keys were tried in following order:\n\t\tIn Environment Variables key ENDPOINT_URL was not found.\nPlease refer to https://dlthub.com/docs/general-usage/credentials for more information\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "print(dlt.secrets[\"endpoint_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb94650-aaee-4b44-ad3c-ec5954085493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinIO URL: s3://warehouse?endpoint=http://minio:9000&access_key=admin&secret_key=password&region=us-east-1\n",
      "pipeline configured\n",
      "Error en el pipeline: Destination s3 is not one of the standard dlt destinations\n"
     ]
    },
    {
     "ename": "UnknownDestinationModule",
     "evalue": "Destination s3 is not one of the standard dlt destinations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m:          VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n0               1  2025-01-01 00:18:38   2025-01-01 00:26:59              1.0   \n1               1  2025-01-01 00:32:40   2025-01-01 00:35:13              1.0   \n2               1  2025-01-01 00:44:04   2025-01-01 00:46:01              1.0   \n3               2  2025-01-01 00:14:27   2025-01-01 00:20:01              3.0   \n4               2  2025-01-01 00:21:34   2025-01-01 00:25:06              3.0   \n...           ...                  ...                   ...              ...   \n3475221         2  2025-01-31 23:01:48   2025-01-31 23:16:29              NaN   \n3475222         2  2025-01-31 23:50:29   2025-02-01 00:17:27              NaN   \n3475223         2  2025-01-31 23:26:59   2025-01-31 23:43:01              NaN   \n3475224         2  2025-01-31 23:14:34   2025-01-31 23:34:52              NaN   \n3475225         2  2025-01-31 23:56:42   2025-02-01 00:07:27              NaN   \n\n         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n0                 1.60         1.0                  N           229   \n1                 0.50         1.0                  N           236   \n2                 0.60         1.0                  N           141   \n3                 0.52         1.0                  N           244   \n4                 0.66         1.0                  N           244   \n...                ...         ...                ...           ...   \n3475221           3.35         NaN               None            79   \n3475222           8.73         NaN               None           161   \n3475223           2.64         NaN               None           144   \n3475224           3.16         NaN               None           142   \n3475225           2.29         NaN               None           237   \n\n         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n0                 237             1        10.00    3.5      0.5        3.00   \n1                 237             1         5.10    3.5      0.5        2.02   \n2                 141             1         5.10    3.5      0.5        2.00   \n3                 244             2         7.20    1.0      0.5        0.00   \n4                 116             2         5.80    1.0      0.5        0.00   \n...               ...           ...          ...    ...      ...         ...   \n3475221           237             0        15.85    0.0      0.5        0.00   \n3475222           116             0        28.14    0.0      0.5        0.00   \n3475223           246             0        14.91    0.0      0.5        0.00   \n3475224           107             0        17.55    0.0      0.5        0.00   \n3475225           238             0        12.09    0.0      0.5        0.00   \n\n         tolls_amount  improvement_surcharge  total_amount  \\\n0                 0.0                    1.0         18.00   \n1                 0.0                    1.0         12.12   \n2                 0.0                    1.0         12.10   \n3                 0.0                    1.0          9.70   \n4                 0.0                    1.0          8.30   \n...               ...                    ...           ...   \n3475221           0.0                    1.0         20.60   \n3475222           0.0                    1.0         32.89   \n3475223           0.0                    1.0         19.66   \n3475224           0.0                    1.0         22.30   \n3475225           0.0                    1.0         16.09   \n\n         congestion_surcharge  Airport_fee  cbd_congestion_fee  \n0                         2.5          0.0                0.00  \n1                         2.5          0.0                0.00  \n2                         2.5          0.0                0.00  \n3                         0.0          0.0                0.00  \n4                         0.0          0.0                0.00  \n...                       ...          ...                 ...  \n3475221                   NaN          NaN                0.75  \n3475222                   NaN          NaN                0.75  \n3475223                   NaN          NaN                0.75  \n3475224                   NaN          NaN                0.75  \n3475225                   NaN          NaN                0.00  \n\n[3475226 rows x 20 columns] is not JSON serializable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:283\u001b[0m, in \u001b[0;36mPipeline.extract\u001b[0;34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# TODO: merge infos for all the sources\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     extract_ids\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# commit extract ids\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# TODO: if we fail here we should probably wipe out the whole extract folder\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:846\u001b[0m, in \u001b[0;36mPipeline._extract_source\u001b[0;34m(self, storage, source, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m    844\u001b[0m source_schema\u001b[38;5;241m.\u001b[39mupdate_normalizers()\n\u001b[0;32m--> 846\u001b[0m extract_id \u001b[38;5;241m=\u001b[39m \u001b[43mextract_with_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# if source schema does not exist in the pipeline\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/extract/extract.py:190\u001b[0m, in \u001b[0;36mextract_with_schema\u001b[0;34m(storage, source, schema, collector, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m    188\u001b[0m             _reset_resource_state(resource\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m--> 190\u001b[0m extractor \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# iterate over all items in the pipeline and update the schema if dynamic table hints were present\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/extract/extract.py:165\u001b[0m, in \u001b[0;36mextract\u001b[0;34m(extract_id, source, storage, collector, max_parallel_items, workers, futures_poll_interval)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# flush all buffered writers\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose_writers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# returns set of partial tables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/storages/data_item_storage.py:41\u001b[0m, in \u001b[0;36mDataItemStorage.close_writers\u001b[0;34m(self, extract_id)\u001b[0m\n\u001b[1;32m     40\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosing writer for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwriter\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and actual name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwriter\u001b[38;5;241m.\u001b[39m_file_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/data_writers/buffered.py:100\u001b[0m, in \u001b[0;36mBufferedDataWriter.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_open()\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_and_close_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/data_writers/buffered.py:135\u001b[0m, in \u001b[0;36mBufferedDataWriter._flush_and_close_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flush_and_close_file\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# if any buffered items exist, flush them\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# if writer exists then close it\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/data_writers/buffered.py:130\u001b[0m, in \u001b[0;36mBufferedDataWriter._flush_items\u001b[0;34m(self, allow_empty_file)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_items:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffered_items\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_items\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/data_writers/writers.py:109\u001b[0m, in \u001b[0;36mJsonlListPUAEncodeWriter.write_data\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# write all rows as one list which will require to write just one line\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# encode types with PUA characters\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtyped_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/json/_orjson.py:24\u001b[0m, in \u001b[0;36mtyped_dump\u001b[0;34m(obj, fp, pretty)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtyped_dump\u001b[39m(obj: Any, fp: IO[\u001b[38;5;28mbytes\u001b[39m], pretty:\u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mtyped_dumpb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/json/_orjson.py:28\u001b[0m, in \u001b[0;36mtyped_dumpb\u001b[0;34m(obj, sort_keys, pretty)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtyped_dumpb\u001b[39m(obj: Any, sort_keys: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, pretty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_pua_encode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOPT_PASSTHROUGH_DATETIME\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/common/json/_orjson.py:16\u001b[0m, in \u001b[0;36m_dumps\u001b[0;34m(obj, sort_keys, pretty, default, options)\u001b[0m\n\u001b[1;32m     15\u001b[0m     options \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m orjson\u001b[38;5;241m.\u001b[39mOPT_SORT_KEYS\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Type is not JSON serializable: DataFrame",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Ejecutar el pipeline\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando ingesta de datos...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m load_info \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43myellow_trip_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader_file_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#destination=dlt.destinations.filesystem(bucket_url=minio_url)\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline ejecutado exitosamente!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:108\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:142\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(ConfigSectionContext(pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections)):\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# extract from the source\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(loader_file_format\u001b[38;5;241m=\u001b[39mloader_file_format)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(destination, dataset_name, credentials\u001b[38;5;241m=\u001b[39mcredentials)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:108\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:82\u001b[0m, in \u001b[0;36mwith_schemas_sync.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlive_schemas:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# refresh live schemas in storage or import schema path\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mcommit_live_schema(name)\n\u001b[0;32m---> 82\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# refresh list of schemas if any new schemas are added\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlist_schemas()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:68\u001b[0m, in \u001b[0;36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_state(extract_state\u001b[38;5;241m=\u001b[39mshould_extract_state) \u001b[38;5;28;01mas\u001b[39;00m state:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# add the state to container as a context\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container\u001b[38;5;241m.\u001b[39minjectable_context(StateInjectableContext(state\u001b[38;5;241m=\u001b[39mstate)):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:142\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(ConfigSectionContext(pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections)):\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/dlt/pipeline/pipeline.py:292\u001b[0m, in \u001b[0;36mPipeline.extract\u001b[0;34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ExtractInfo(describe_extract_data(data))\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# TODO: provide metrics from extractor\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc, ExtractInfo(describe_extract_data(data))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m: Pipeline execution failed at stage extract with exception:\n\n<class 'TypeError'>\nType is not JSON serializable: DataFrame"
     ]
    }
   ],
   "source": [
    "# Configuración de la URL\n",
    "URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "\n",
    "@dlt.resource(name=\"yellow_trip_data\", write_disposition=\"replace\")\n",
    "def yellow_trip_data():\n",
    "    try:\n",
    "        print(\"Descargando archivo parquet...\")\n",
    "        response = requests.get(URL, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".parquet\") as tmp:\n",
    "            tmp.write(response.content)\n",
    "            tmp.flush()\n",
    "\n",
    "            # Leer en formato Arrow\n",
    "            table = pq.read_table(tmp.name)\n",
    "            print(f\"Datos cargados en Arrow: {table.num_rows} filas, {table.num_columns} columnas\")\n",
    "            yield table\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar o procesar datos: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Configuración de MinIO\n",
    "        minio_endpoint = os.getenv('MINIO_ENDPOINT', 'minio:9000')  # Cambiado a 'minio' para Docker\n",
    "        bucket_name = os.getenv('BUCKET_NAME', 'warehouse')         # Usando el bucket existente\n",
    "        access_key = os.getenv('MINIO_ACCESS_KEY', 'admin')\n",
    "        secret_key = os.getenv('MINIO_SECRET_KEY', 'password')\n",
    "        \n",
    "        # Construir la URL de MinIO (S3 compatible)\n",
    "        minio_url = f\"s3://{bucket_name}?endpoint=http://{minio_endpoint}&access_key={access_key}&secret_key={secret_key}&region=us-east-1\"\n",
    "        print(f\"MinIO URL: {minio_url}\")\n",
    "        print(\"pipeline configured\")\n",
    "        # Configurar pipeline\n",
    "        pipeline = dlt.pipeline(\n",
    "            pipeline_name=\"yellow_trip_data_pipeline\",\n",
    "            destination='filesystem',\n",
    "            dataset_name=\"yellow_trip\",\n",
    "            full_refresh=True,\n",
    "        )\n",
    "        \n",
    "        # Ejecutar el pipeline\n",
    "        print(\"Iniciando ingesta de datos...\")\n",
    "        load_info = pipeline.run(\n",
    "            yellow_trip_data(),\n",
    "            loader_file_format=\"parquet\",\n",
    "            #destination=dlt.destinations.filesystem(bucket_url=minio_url)\n",
    "        )\n",
    "        \n",
    "        print(\"Pipeline ejecutado exitosamente!\")\n",
    "        print(f\"Load info: {load_info}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en el pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff7b9c-da40-4c44-98d4-f3a19e921522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
